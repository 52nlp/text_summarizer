Applied attention on Sequence to Sequence model for the task of Text Summarizer using Tensorflow's raw_rnn. here, I haven't used Tensorflow's inbuilt seq2seq function. The reason behind is to apply attention mechanism manually.
Loss convergence and output can be seen from "final_output.txt"
Though the loss convergence is not great but this gives the idea of how attention works in a Sequence to Sequence model.
